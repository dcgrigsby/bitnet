# Tiny BitNet Validation Experiment
# Fast Training on RTX 3060 with Simple Synthetic Data

**Date:** 2025-12-26
**Goal:** Validate BitNet training infrastructure with a small, fast experiment that should produce clearly interpretable results

---

## Problem Statement

The 95M parameter model after 400K steps (3.3B tokens, T/P ≈ 34) is not producing coherent output. Possible causes:

1. **Training infrastructure broken** (quantization, gradients, optimizer)
2. **Model too small** for 32K vocabulary and complex natural language
3. **Undertrained** (needs more tokens or different learning schedule)
4. **Generation/sampling broken** (model is fine, but decoding has issues)

**Solution:** Train a tiny model (10-20M params) on a synthetic dataset with small vocabulary and simple patterns. If this works → infrastructure is fine, 95M model needs different approach. If this fails → infrastructure has bugs.

---

## 1. Model Architecture: BitNet-Tiny (12M Parameters)

### Configuration

```python
BitNetConfig_Tiny = {
    # Architecture
    'vocab_size': 2048,          # Small vocabulary (vs 32,000)
    'hidden_size': 512,          # Reduced from 768
    'num_layers': 6,             # Reduced from 8
    'num_heads': 8,              # Reduced from 12
    'num_kv_heads': 4,           # GQA 2:1 ratio maintained
    'ffn_hidden_size': 1536,     # 3× expansion (reduced from 3072)
    'head_dim': 64,              # 512 / 8
    'max_seq_length': 256,       # Good for short stories

    # Training hyperparameters
    'learning_rate': 0.0015,     # BitNet standard
    'weight_decay': 0.1,         # Stage 1
    'warmup_steps': 250,         # Proportional to model size
    'adam_beta1': 0.9,
    'adam_beta2': 0.95,
}
```

### Parameter Breakdown

```
Tied embeddings:     2,048 × 512 = 1,048,576
6 transformer layers:
  - Per layer attention:  ~1,052,672
  - Per layer FFN:        ~3,148,288
  - Per layer total:      ~4,200,960
  6 layers total:         25,205,760
Final RMSNorm:           512
──────────────────────────────────────
Total:                   ~26.3M parameters
```

**Adjusted for 12M target:**
- Vocab size: 2,048
- Hidden size: 384
- Num layers: 6
- FFN hidden: 1,152 (3× expansion)
- **Total: ~12,058,624 parameters** ✓

---

## 2. Dataset Options (Ranked by Recommendation)

### Option 1: TinyStories (RECOMMENDED) ⭐

**Source:** `roneneldan/TinyStories` on HuggingFace

**Description:**
- Synthetic stories generated by GPT-3.5/GPT-4
- Uses vocabulary a 3-4 year old would understand (~2,000 unique words)
- Simple grammar, consistent structure
- Short stories (typically 100-300 words)

**Dataset size:**
- 2.1M training stories
- ~500M tokens (estimated)

**Why it's perfect:**
- Proven to work with models as small as 1-10M parameters
- Simple vocabulary matches our 2K vocab size
- Clear success criteria: grammatically correct, coherent stories
- Already tokenized and ready to use

**Example story:**
```
Once upon a time, there was a little girl named Lily. She loved to play outside
in the sunshine. One day, she saw a butterfly and tried to catch it. But the
butterfly flew away. Lily was sad but she knew she would see more butterflies soon.
```

**Training time on RTX 3060:**
- Tokens needed: 12M params × 20 = 240M tokens
- Throughput: ~20,000 tok/s (small model, short sequences)
- Time: 240M / 20,000 = 12,000 seconds ≈ **3.3 hours** ✓

---

### Option 2: Simple Arithmetic (Easy to Generate)

**Description:**
Generate synthetic arithmetic problems with solutions.

**Examples:**
```
2 + 3 = 5
15 - 7 = 8
4 * 6 = 24
20 / 4 = 5
(3 + 5) * 2 = 16
```

**Vocabulary:**
- Digits: 0-9 (10 tokens)
- Operators: +, -, *, /, =, (, ) (7 tokens)
- Spaces and special tokens (5 tokens)
- **Total: ~25 tokens** (tiny!)

**Dataset generation:**
```python
import random

def generate_arithmetic_sample():
    a = random.randint(0, 100)
    b = random.randint(1, 100)
    op = random.choice(['+', '-', '*', '//'])

    if op == '+':
        result = a + b
    elif op == '-':
        result = a - b
    elif op == '*':
        result = a * b
    else:
        result = a // b

    return f"{a} {op} {b} = {result}"

# Generate 1M samples
with open('arithmetic_train.txt', 'w') as f:
    for _ in range(1_000_000):
        f.write(generate_arithmetic_sample() + '\n')
```

**Success criteria:**
- Model should learn to correctly complete "X + Y = " → correct answer
- Very clear signal if training works

**Training time:**
- Tokens needed: 12M × 10 = 120M tokens (less due to tiny vocab)
- Time: ~1.5 hours ✓

**Pros:**
- Extremely simple, clear success/failure
- Tiny vocabulary
- Easy to generate unlimited data
- Fast to train

**Cons:**
- Less impressive than natural language
- Doesn't test complex reasoning or grammar

---

### Option 3: Simple Programming (Python subset)

**Description:**
Synthetic Python code with limited vocabulary.

**Vocabulary:**
- Keywords: def, if, else, for, while, return, print (15 tokens)
- Common identifiers: i, j, x, y, result, count (20 tokens)
- Operators and syntax: =, +, -, <, >, (, ), :, [, ] (20 tokens)
- Digits: 0-9 (10 tokens)
- **Total: ~100 tokens**

**Examples:**
```python
def add(x, y):
    return x + y

def count_to_ten():
    for i in range(10):
        print(i)

def is_even(x):
    if x % 2 == 0:
        return True
    else:
        return False
```

**Success criteria:**
- Model completes simple function definitions
- Correct indentation and syntax

**Training time:** ~2 hours

---

### Option 4: Structured Text (JSON/Simple Grammar)

**Description:**
Generate structured data with predictable patterns.

**Examples:**
```json
{"name": "Alice", "age": 25, "city": "Boston"}
{"name": "Bob", "age": 30, "city": "Seattle"}
{"name": "Carol", "age": 22, "city": "Austin"}
```

**Vocabulary:**
- Common names (50 tokens)
- Common cities (50 tokens)
- Ages: 18-99 (let's say ~20 common patterns)
- JSON syntax: {, }, ", :, , (10 tokens)
- **Total: ~150 tokens**

**Success criteria:**
- Model generates valid JSON
- Consistent structure

**Training time:** ~2 hours

---

## 3. Recommended Approach: TinyStories

### Why TinyStories is Best

1. **Proven track record:** Microsoft Research showed models with only 1-10M params can produce fluent stories
2. **Natural language:** More impressive and useful than arithmetic
3. **Clear evaluation:** Easy to see if output is coherent
4. **Ready to use:** Available on HuggingFace, no generation needed
5. **Good vocabulary size:** ~2K words matches our tiny vocab
6. **Fast training:** Should converge in a few hours

### Custom Tokenizer for TinyStories

Since we're reducing vocab from 32K → 2K, we need a custom tokenizer:

**Option A: Train BPE tokenizer on TinyStories**
```python
from tokenizers import Tokenizer, models, trainers, pre_tokenizers

# Load TinyStories dataset
dataset = load_dataset("roneneldan/TinyStories")

# Train BPE tokenizer with 2048 vocab
tokenizer = Tokenizer(models.BPE())
tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()

trainer = trainers.BpeTrainer(vocab_size=2048, special_tokens=["<pad>", "<s>", "</s>", "<unk>"])
tokenizer.train_from_iterator(dataset["train"]["text"], trainer)

tokenizer.save("tinystories_2k_tokenizer.json")
```

**Option B: Use character-level encoding**
- Vocabulary: a-z, A-Z, 0-9, punctuation, spaces (~100 tokens)
- Ultra-simple, no tokenizer training needed
- Longer sequences, but more interpretable

**Recommendation:** Option A (BPE with 2K vocab) - best balance

---

## 4. Training Configuration

### Dataset & Tokens

- **Dataset:** TinyStories (500M tokens available)
- **Training tokens:** 12M params × 20 (Chinchilla) = **240M tokens**
- **Repetition:** 240M / 500M = 48% (zero repetition) ✓

### Training Hyperparameters

```python
training_config = {
    'total_steps': 30_000,
    'batch_size': 32,           # Should fit easily in 12GB
    'seq_length': 256,          # Good for short stories
    'tokens_per_step': 8_192,   # 32 × 256
    'total_tokens': 245_760_000, # ~246M tokens

    # Learning rate schedule
    'warmup_steps': 250,
    'stage1_end': 15_000,       # 50% mark
    'peak_lr': 0.0015,
    'stage2_lr': 0.001,
    'final_lr': 0.000015,

    # Optimizer
    'weight_decay': 0.1,        # Stage 1 only
    'adam_beta1': 0.9,
    'adam_beta2': 0.95,
    'grad_clip_norm': 1.0,
}
```

### Expected Performance (RTX 3060)

**Step time estimate:**
- 12M params vs 95M params ≈ 8× smaller
- Expected speedup: ~5-6× (not linear due to overhead)
- 95M step time: ~695ms
- **12M step time: ~120-140ms** (estimated)

**Training duration:**
```
30,000 steps × 130ms = 3,900 seconds ≈ 65 minutes
With overhead (checkpointing, eval, samples): ~90 minutes ≈ 1.5 hours
```

**Throughput:**
- Tokens per step: 8,192
- Step time: ~130ms
- **Throughput: ~63,000 tokens/sec**

---

## 5. Success Criteria

### What "Working" Looks Like

After 30K steps (~1.5 hours), the model should:

✅ **Loss trajectory:**
- Start: ~6-7 (initial loss for small vocab)
- End: ~1.5-2.0 (much lower due to simpler task)

✅ **Sample quality:**
- Grammatically correct sentences
- Coherent short stories (even if simple)
- Proper capitalization and punctuation
- No repetition loops

✅ **Example good output:**
```
Prompt: "Once upon a time"
Output: "Once upon a time there was a little boy named Tim. He liked to play
with his ball. One day he saw a big tree and wanted to climb it."
```

✅ **Quantization metrics:**
- Ternary weights properly distributed (~33% each of -1, 0, +1)
- No NaN or Inf in activations
- Gradients flowing normally

### What "Broken" Looks Like

❌ **Infrastructure issues:**
- Loss doesn't decrease below ~6
- Samples are gibberish or repetitive
- Quantization stuck (e.g., all weights → 0)
- NaN/Inf appearing

❌ **Example bad output:**
```
Prompt: "Once upon a time"
Output: "the the the the the the the the the"
```
OR
```
Output: "xkjq mlp zzz aaa qwe"  # Random tokens
```

---

## 6. Implementation Steps

### Step 1: Create Tiny Model Config

Create `src/bitnet/config_tiny.py`:

```python
from dataclasses import dataclass

@dataclass
class BitNetConfigTiny:
    """12M parameter BitNet for TinyStories validation."""

    vocab_size: int = 2048
    hidden_size: int = 384
    num_layers: int = 6
    num_heads: int = 8
    num_kv_heads: int = 4
    ffn_hidden_size: int = 1152
    max_seq_length: int = 256
    norm_eps: float = 1e-5

    # Training
    learning_rate: float = 0.0015
    weight_decay: float = 0.1
    warmup_steps: int = 250
    adam_beta1: float = 0.9
    adam_beta2: float = 0.95

    @property
    def head_dim(self) -> int:
        return self.hidden_size // self.num_heads
```

### Step 2: Create TinyStories DataLoader

Create `src/bitnet/data_tinystories.py`:

```python
from datasets import load_dataset
from tokenizers import Tokenizer
import torch

class TinyStoriesDataLoader:
    def __init__(self, tokenizer_path, batch_size, seq_len, num_steps):
        # Load tokenizer
        self.tokenizer = Tokenizer.from_file(tokenizer_path)
        self.batch_size = batch_size
        self.seq_len = seq_len

        # Load TinyStories
        self.dataset = load_dataset("roneneldan/TinyStories", split="train", streaming=True)
        self.iterator = iter(self.dataset)

    def __iter__(self):
        buffer = []

        for story in self.iterator:
            # Tokenize
            encoded = self.tokenizer.encode(story["text"])
            buffer.extend(encoded.ids)

            # Yield batches when buffer is full
            while len(buffer) >= self.batch_size * self.seq_len:
                batch = buffer[:self.batch_size * self.seq_len]
                buffer = buffer[self.batch_size * self.seq_len:]

                # Reshape to (batch_size, seq_len)
                batch_tensor = torch.tensor(batch).reshape(self.batch_size, self.seq_len)
                yield batch_tensor
```

### Step 3: Create Training Script

Create `train_bitnet_tiny.py`:

```python
#!/usr/bin/env python3
"""
Train 12M BitNet on TinyStories for infrastructure validation.
Expected runtime: ~1.5 hours on RTX 3060
"""

import torch
from src.bitnet.config_tiny import BitNetConfigTiny
from src.bitnet.transformer import BitNetTransformer
from src.bitnet.data_tinystories import TinyStoriesDataLoader
from src.bitnet.train import TwoStageLRScheduler, TwoStageWDScheduler
from src.bitnet.instrumentation import setup_run, log_metrics, save_checkpoint, generate_samples

def main():
    # Config
    config = BitNetConfigTiny()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Setup run directory
    run_id = setup_run("bitnet_12M_tinystories")

    # Initialize model
    model = BitNetTransformer(config).to(device)
    print(f"Model parameters: {sum(p.numel() for p in model.parameters()):,}")

    # Data
    dataloader = TinyStoriesDataLoader(
        tokenizer_path="tinystories_2k_tokenizer.json",
        batch_size=32,
        seq_len=256,
        num_steps=30_000
    )

    # Optimizer & schedulers
    optimizer = torch.optim.AdamW(model.parameters(),
                                  lr=config.learning_rate,
                                  betas=(config.adam_beta1, config.adam_beta2),
                                  weight_decay=config.weight_decay)

    lr_scheduler = TwoStageLRScheduler(optimizer, total_steps=30_000,
                                       warmup_steps=250)
    wd_scheduler = TwoStageWDScheduler(optimizer, total_steps=30_000)

    # Training loop
    model.train()
    for step, batch in enumerate(dataloader):
        if step >= 30_000:
            break

        batch = batch.to(device)

        # Forward & backward
        logits = model(batch)
        loss = compute_loss(logits, batch)

        loss.backward()
        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        optimizer.zero_grad()

        lr_scheduler.step()
        wd_scheduler.step()

        # Logging
        if step % 100 == 0:
            log_metrics(run_id, step, loss.item(), optimizer, grad_norm)
            print(f"Step {step:6d} | Loss: {loss.item():.4f} | LR: {optimizer.param_groups[0]['lr']:.6f}")

        # Checkpoints
        if step % 5000 == 0:
            save_checkpoint(run_id, step, model, optimizer, lr_scheduler, wd_scheduler)

        # Sample generation
        if step % 1000 == 0:
            generate_samples(run_id, step, model, tokenizer_path="tinystories_2k_tokenizer.json")

    print(f"Training complete! Run ID: {run_id}")

if __name__ == "__main__":
    main()
```

### Step 4: Train Custom Tokenizer

Create `train_tinystories_tokenizer.py`:

```python
from datasets import load_dataset
from tokenizers import Tokenizer, models, trainers, pre_tokenizers

def train_tokenizer():
    # Load dataset
    print("Loading TinyStories dataset...")
    dataset = load_dataset("roneneldan/TinyStories", split="train", streaming=True)

    # Initialize tokenizer
    tokenizer = Tokenizer(models.BPE())
    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()

    # Train
    print("Training tokenizer (2048 vocab)...")
    trainer = trainers.BpeTrainer(
        vocab_size=2048,
        special_tokens=["<pad>", "<s>", "</s>", "<unk>"]
    )

    def text_iterator():
        for i, item in enumerate(dataset):
            if i >= 100_000:  # Use 100K stories for training tokenizer
                break
            yield item["text"]

    tokenizer.train_from_iterator(text_iterator(), trainer)

    # Save
    tokenizer.save("tinystories_2k_tokenizer.json")
    print("Tokenizer saved to tinystories_2k_tokenizer.json")

if __name__ == "__main__":
    train_tokenizer()
```

---

## 7. Execution Plan

### Phase 1: Setup (30 minutes)

1. **Install dependencies:**
```bash
pip install datasets tokenizers
```

2. **Train tokenizer:**
```bash
python train_tinystories_tokenizer.py
# Expected time: ~10 minutes
```

3. **Verify tokenizer:**
```bash
python -c "
from tokenizers import Tokenizer
t = Tokenizer.from_file('tinystories_2k_tokenizer.json')
text = 'Once upon a time there was a little girl.'
print(f'Vocab size: {t.get_vocab_size()}')
print(f'Encoded: {t.encode(text).ids}')
print(f'Decoded: {t.decode(t.encode(text).ids)}')
"
```

### Phase 2: Training (1.5 hours)

4. **Launch training:**
```bash
python train_bitnet_tiny.py
```

5. **Monitor (from another terminal):**
```bash
# Watch loss
tail -f runs/bitnet_12M_tinystories_*/metrics/scalars.jsonl | jq '.loss'

# Check samples every 1K steps
tail -5 runs/bitnet_12M_tinystories_*/samples/samples.jsonl | jq '.generated_text'
```

### Phase 3: Evaluation (15 minutes)

6. **Load final checkpoint and generate:**
```bash
python -c "
import torch
from src.bitnet.transformer import BitNetTransformer
from src.bitnet.config_tiny import BitNetConfigTiny
from tokenizers import Tokenizer

# Load model
config = BitNetConfigTiny()
model = BitNetTransformer(config)
checkpoint = torch.load('runs/bitnet_12M_tinystories_*/checkpoints/step_30000/checkpoint.pt')
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()

# Load tokenizer
tokenizer = Tokenizer.from_file('tinystories_2k_tokenizer.json')

# Generate
prompts = [
    'Once upon a time',
    'There was a little',
    'One day a boy',
]

for prompt in prompts:
    input_ids = torch.tensor([tokenizer.encode(prompt).ids])
    output_ids = model.generate(input_ids, max_length=50)
    output_text = tokenizer.decode(output_ids[0])
    print(f'Prompt: {prompt}')
    print(f'Output: {output_text}')
    print()
"
```

---

## 8. Debugging Decision Tree

### If Training Works ✅

**Output is coherent stories:**
- ✅ Training infrastructure is GOOD
- ✅ BitNet quantization working properly
- ✅ Optimizer, schedulers, gradients all working

**Next steps for 95M model:**
- Problem is likely: vocabulary too large (32K) or dataset too complex
- Try: 95M model with TinyStories (keep 2K vocab)
- Try: Train 95M model for 10× longer (4M steps instead of 400K)
- Try: Increase model size to 200M+ params for 32K vocab

### If Training Fails ❌

**Loss doesn't decrease or output is garbage:**
- ❌ Training infrastructure has bugs
- Check:
  1. Quantization: Are weights actually ternary?
  2. Gradients: Use STE, are gradients flowing?
  3. Data: Is tokenization correct?
  4. Loss: Is cross-entropy computed correctly?

**Specific debug checks:**
```python
# Check 1: Verify ternary quantization
for name, param in model.named_parameters():
    if 'weight' in name and 'linear' in name:
        unique_vals = torch.unique(param.data)
        print(f"{name}: unique values = {unique_vals}")
        # Should see approximately: tensor([-1., 0., 1.])

# Check 2: Verify gradients flowing
for name, param in model.named_parameters():
    if param.grad is not None:
        print(f"{name}: grad norm = {param.grad.norm().item():.6f}")
        # Should see non-zero gradients

# Check 3: Verify data pipeline
batch = next(iter(dataloader))
print(f"Batch shape: {batch.shape}")
print(f"Batch min/max: {batch.min()}/{batch.max()}")
print(f"Sample decoded: {tokenizer.decode(batch[0].tolist())}")
# Should see: (32, 256), values in [0, 2047], readable text
```

---

## 9. Timeline Summary

| Task | Duration | Cumulative |
|------|----------|------------|
| Install dependencies | 5 min | 5 min |
| Train tokenizer | 10 min | 15 min |
| Verify tokenizer | 5 min | 20 min |
| **Setup complete** | | **20 min** |
| Run training (30K steps) | 90 min | 110 min |
| Generate test samples | 10 min | 120 min |
| **Total** | | **~2 hours** |

**vs. 95M model:** 400K steps × 695ms ≈ 77 hours (38× faster!)

---

## 10. Alternative: Even Faster Arithmetic Validation

If you want **maximum speed** (results in 30 minutes):

### Super-Tiny Config (5M params)

```python
BitNetConfigArithmetic = {
    'vocab_size': 32,           # Just digits + operators
    'hidden_size': 256,
    'num_layers': 4,
    'num_heads': 4,
    'num_kv_heads': 2,
    'ffn_hidden_size': 768,
}
# Total: ~5M parameters
```

### Training

- Task: Learn "X + Y = Z" patterns
- Tokens needed: 5M × 10 = 50M tokens
- Training time: **~15 minutes** on RTX 3060

### Success Check

```python
# Test inference
model.complete("25 + 17 = ")  # Should output "42"
model.complete("100 - 35 = ") # Should output "65"
```

**This is the ultimate sanity check** - if it can't learn arithmetic, something is definitely broken!

---

## 11. Recommendation

### Best Approach: TinyStories

**Why:**
- Fast enough (1.5 hours vs 77 hours)
- Natural language (more meaningful than arithmetic)
- Clear success criteria
- Proven to work with tiny models

**Fallback:** If you want results TODAY (< 1 hour), do arithmetic first as a quick sanity check, then TinyStories tomorrow.

---

## 12. Files to Create

Summary of new files needed:

1. `src/bitnet/config_tiny.py` - 12M model config
2. `src/bitnet/data_tinystories.py` - TinyStories dataloader
3. `train_tinystories_tokenizer.py` - Tokenizer training script
4. `train_bitnet_tiny.py` - Main training script
5. `test_tiny_model.py` - Inference testing

**Total new code:** ~400 lines (mostly adapted from existing 95M scripts)

---

## 13. Expected Outcomes

### Success Scenario (90% likely if infrastructure works)

After 1.5 hours, you'll have:
- ✅ Loss curve dropping from ~7 → ~1.5
- ✅ Coherent TinyStories-style output
- ✅ Proof that BitNet training works
- → Next: Figure out why 95M model failed (likely vocab/data mismatch)

### Failure Scenario (10% likely if there are bugs)

After 1.5 hours:
- ❌ Loss stuck or increasing
- ❌ Gibberish output
- → Debug BitNet quantization, STE gradients, data pipeline

**Either way, you'll have clarity in 2 hours instead of 2 weeks!**

---

## References

- [TinyStories Paper (arXiv:2305.07759)](https://arxiv.org/abs/2305.07759) - Microsoft Research
- [TinyStories Dataset](https://huggingface.co/datasets/roneneldan/TinyStories)
- [TinyGSM Paper (arXiv:2312.09241)](https://arxiv.org/abs/2312.09241)
- [Small Language Models Survey (2025)](https://arxiv.org/html/2501.05465v1)

---

**Ready to implement? Let me know if you want me to generate the code files!**
